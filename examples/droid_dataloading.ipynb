{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "\n",
    "from robot_world.data.rlds_utils import droid_dataset_transform, robomimic_transform, TorchRLDSDataset\n",
    "\n",
    "from octo.data.dataset import make_dataset_from_rlds, make_interleaved_dataset\n",
    "from octo.data.utils.data_utils import combine_dataset_statistics\n",
    "from octo.utils.spec import ModuleSpec\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Get Dataset Information ------------------------------\n",
    "DATA_PATH = \"\"              # UPDATE WITH PATH TO RLDS DATASETS\n",
    "DATASET_NAMES = [\"droid_100\"]   # You can add additional co-training datasets here, swicth between \"droid\" or \"droid_100\"\n",
    "sample_weights = [1]        # Add to this if you add additional co-training datasets\n",
    "\n",
    "# ------------------------------ Construct Dataset ------------------------------\n",
    "BASE_DATASET_KWARGS = {\n",
    "    \"data_dir\": DATA_PATH,\n",
    "    \"image_obs_keys\": {\"primary\": \"exterior_image_1_left\", \"secondary\": \"exterior_image_2_left\"},\n",
    "    \"state_obs_keys\": [\"cartesian_position\", \"gripper_position\"],\n",
    "    \"language_key\": \"language_instruction\",\n",
    "    \"norm_skip_keys\":  [\"proprio\"],\n",
    "    \"action_proprio_normalization_type\": \"bounds\",\n",
    "    \"absolute_action_mask\": [True] * 10,                    # droid_dataset_transform uses absolute actions\n",
    "    \"action_normalization_mask\": [True] * 9 + [False],      # don't normalize final (gripper) dimension\n",
    "    \"standardize_fn\": droid_dataset_transform,\n",
    "}\n",
    "\n",
    "# By default, only use success trajectories in DROID\n",
    "filter_functions = [\n",
    "    [\n",
    "        ModuleSpec.create(\n",
    "            \"robot_world.data.rlds_utils:filter_success\"\n",
    "        )\n",
    "    ] if d_name == \"droid\" else [] for d_name in DATASET_NAMES\n",
    "]\n",
    "dataset_kwargs_list = [\n",
    "    {\n",
    "        \"name\": d_name,\n",
    "        \"filter_functions\": f_functions,\n",
    "        **BASE_DATASET_KWARGS\n",
    "    }\n",
    "    for d_name, f_functions in zip(DATASET_NAMES, filter_functions)\n",
    "]\n",
    "\n",
    "# Compute combined normalization stats. Note: can also set this to None to normalize each dataset separately\n",
    "combined_dataset_statistics = combine_dataset_statistics(\n",
    "    [make_dataset_from_rlds(**dataset_kwargs, train=True)[1] for dataset_kwargs in dataset_kwargs_list]\n",
    ")\n",
    "\n",
    "dataset = make_interleaved_dataset(\n",
    "    dataset_kwargs_list,\n",
    "    sample_weights,\n",
    "    train=True,\n",
    "    shuffle_buffer_size=10000,         # adjust this based on your system RAM\n",
    "    batch_size=None,                    # batching will be handled in PyTorch Dataloader object\n",
    "    balance_weights=False,\n",
    "    dataset_statistics=combined_dataset_statistics,\n",
    "    traj_transform_kwargs=dict(\n",
    "        window_size=2,\n",
    "        future_action_window_size=15,\n",
    "        subsample_length=100,\n",
    "        skip_unlabeled=True,            # skip all trajectories without language annotation\n",
    "    ),\n",
    "    frame_transform_kwargs=dict(\n",
    "        image_augment_kwargs=dict(\n",
    "        ),\n",
    "        resize_size=dict(\n",
    "            primary=[360, 640],\n",
    "            secondary=[360, 640],\n",
    "        ),\n",
    "        num_parallel_calls=16,\n",
    "    ),\n",
    "    traj_transform_threads=16,\n",
    "    traj_read_threads=16,\n",
    ")\n",
    "\n",
    "dataset = dataset.map(robomimic_transform, num_parallel_calls=16)\n",
    "\n",
    "# ------------------------------ Create Dataloader ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dataset = TorchRLDSDataset(dataset)\n",
    "train_loader = DataLoader(\n",
    "    pytorch_dataset,\n",
    "    batch_size=128,\n",
    "    num_workers=0,  # important to keep this to 0 so PyTorch does not mess with the parallelism\n",
    ")\n",
    "\n",
    "for i, sample in tqdm.tqdm(enumerate(train_loader)):\n",
    "    if i == 5000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_sample(sample):\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(key, value.shape)\n",
    "        elif isinstance(value, dict):\n",
    "            print(key)\n",
    "            traverse_sample(value)\n",
    "        else:\n",
    "            print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=next(iter(train_loader))\n",
    "traverse_sample(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot-world",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
